{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gWBLj3qxdyF"
      },
      "outputs": [],
      "source": [
        "# ðŸ“Œ Instalar dependencias (si no estÃ¡n instaladas)\n",
        "!pip install transformers torch scikit-learn numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Cargar modelos de Hugging Face\n",
        "modelo_emb_name = \"distilbert-base-uncased\"\n",
        "modelo_resp_name = \"facebook/blenderbot-400M-distill\"\n",
        "\n",
        "tokenizer_emb = AutoTokenizer.from_pretrained(modelo_emb_name)\n",
        "modelo_emb = AutoModel.from_pretrained(modelo_emb_name)\n",
        "\n",
        "tokenizer_resp = AutoTokenizer.from_pretrained(modelo_resp_name)\n",
        "modelo_resp = AutoModelForSeq2SeqLM.from_pretrained(modelo_resp_name)\n",
        "\n",
        "# Cargar el dataset del chatbot (intents.json)\n",
        "archivo_intents = \"intents.json\"\n",
        "try:\n",
        "    with open(archivo_intents, \"r\", encoding=\"utf-8\") as file:\n",
        "        intents = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    intents = {\"intents\": []}\n",
        "\n",
        "# Cargar dataset de celulares\n",
        "archivo_celulares = \"celulares_limpios.json\"\n",
        "try:\n",
        "    with open(archivo_celulares, \"r\", encoding=\"utf-8\") as file:\n",
        "        celulares = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    celulares = []\n",
        "\n",
        "# Memoria de la conversaciÃ³n (para recordar contexto)\n",
        "memoria_chat = []\n",
        "\n",
        "def obtener_embedding(frase):\n",
        "    tokens = tokenizer_emb(frase, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        salida = modelo_emb(**tokens)\n",
        "    return salida.last_hidden_state[:, 0, :].numpy().flatten()\n",
        "\n",
        "patterns, labels = [], []\n",
        "\n",
        "for intent in intents[\"intents\"]:\n",
        "    if \"tag\" in intent and \"patterns\" in intent:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            patterns.append(pattern)\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "if patterns:\n",
        "    X = np.array([obtener_embedding(frase) for frase in patterns])\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(labels)\n",
        "\n",
        "    classifier = SVC(kernel=\"linear\", probability=True)\n",
        "    classifier.fit(X, y)\n",
        "else:\n",
        "    classifier = None\n",
        "\n",
        "def predecir_intencion(frase):\n",
        "    if classifier:\n",
        "        embedding = obtener_embedding(frase)\n",
        "        prediccion = classifier.predict([embedding])[0]\n",
        "        return label_encoder.inverse_transform([prediccion])[0]\n",
        "    return None\n",
        "\n",
        "def obtener_respuesta_json(intencion):\n",
        "    for intent in intents[\"intents\"]:\n",
        "        if intent[\"tag\"] == intencion:\n",
        "            return np.random.choice(intent[\"responses\"])\n",
        "    return None\n",
        "\n",
        "def generar_respuesta_transformer(frase):\n",
        "    tokens = tokenizer_resp(frase, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        respuesta_ids = modelo_resp.generate(**tokens)\n",
        "    return tokenizer_resp.decode(respuesta_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def respuesta_valida(respuesta):\n",
        "    palabras_invalidas = [\"no sÃ©\", \"no entiendo\", \"no puedo ayudar\", \"no tengo informaciÃ³n\"]\n",
        "    return not any(palabra in respuesta.lower() for palabra in palabras_invalidas)\n",
        "\n",
        "def guardar_aprendizaje(nueva_pregunta, nueva_respuesta):\n",
        "    global intents\n",
        "\n",
        "    if not respuesta_valida(nueva_respuesta):\n",
        "        return\n",
        "\n",
        "    intencion_predicha = predecir_intencion(nueva_pregunta)\n",
        "\n",
        "    if intencion_predicha:\n",
        "        for intent in intents[\"intents\"]:\n",
        "            if intent[\"tag\"] == intencion_predicha:\n",
        "                if nueva_pregunta not in intent[\"patterns\"]:\n",
        "                    intent[\"patterns\"].append(nueva_pregunta)\n",
        "                if nueva_respuesta not in intent[\"responses\"]:\n",
        "                    intent[\"responses\"].append(nueva_respuesta)\n",
        "                break\n",
        "    else:\n",
        "        nueva_categoria = f\"aprendizaje_{len(intents['intents'])+1}\"\n",
        "        intents[\"intents\"].append({\n",
        "            \"tag\": nueva_categoria,\n",
        "            \"patterns\": [nueva_pregunta],\n",
        "            \"responses\": [nueva_respuesta]\n",
        "        })\n",
        "\n",
        "    with open(archivo_intents, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(intents, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "def manejar_contexto(texto):\n",
        "    global memoria_chat\n",
        "    memoria_chat.append(texto)\n",
        "    if len(memoria_chat) > 3:\n",
        "        memoria_chat.pop(0)\n",
        "    return \" \".join(memoria_chat)\n",
        "\n",
        "def responder_chatbot(texto):\n",
        "    texto_con_contexto = manejar_contexto(texto)\n",
        "    intencion = predecir_intencion(texto_con_contexto)\n",
        "    respuesta = obtener_respuesta_json(intencion)\n",
        "\n",
        "    if respuesta:\n",
        "        return respuesta\n",
        "\n",
        "    for celular in celulares:\n",
        "        if any(palabra in texto.lower() for palabra in [celular[\"Company Name\"].lower(), celular[\"Model Name\"].lower()]):\n",
        "            return json.dumps(celular, indent=4)\n",
        "\n",
        "    respuesta_generada = generar_respuesta_transformer(texto_con_contexto)\n",
        "\n",
        "    if respuesta_valida(respuesta_generada):\n",
        "        guardar_aprendizaje(texto, respuesta_generada)\n",
        "\n",
        "    return respuesta_generada\n",
        "\n",
        "while True:\n",
        "    pregunta = input(\" Escribe tu consulta (o 'salir' para terminar): \")\n",
        "    if pregunta.lower() == \"salir\":\n",
        "        break\n",
        "    print(\"\", responder_chatbot(pregunta))"
      ],
      "metadata": {
        "id": "FwCTPyMHxe1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}